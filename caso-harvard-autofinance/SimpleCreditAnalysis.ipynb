{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e300cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3c216",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "data = pd.read_excel(\"IMB469-XLS-ENG (2).xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1694089",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\n",
    "    columns=[\"Agmt No\", \"Contract Status\", \"Start_Date\", \"DATASET\", \"DefaulterFlag\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb44c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"AGE\",\n",
    "    \"NOOFDEPE\",\n",
    "    \"MTHINCTH\",\n",
    "    \"SALDATFR\",\n",
    "    \"TENORYR\",\n",
    "    \"DWNPMFR\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa58ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"Region\",\n",
    "    \"Branch\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(\n",
    "    data, test_size=0.2, random_state=42, stratify=data[\"DefaulterType\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d99c0",
   "metadata": {},
   "source": [
    "### Step 2: Feature Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the scaler on numeric features\n",
    "train_data_scaled = train_data.copy()\n",
    "train_data_scaled[numeric_features] = scaler.fit_transform(train_data[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the scaler on numeric features\n",
    "test_data_scaled = test_data.copy()\n",
    "test_data_scaled[numeric_features] = scaler.transform(test_data[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99efc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to one-hot encoded columns\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "encoded_data = one_hot_encoder.fit_transform(train_data_scaled[categorical_features])\n",
    "\n",
    "# Convert the encoded data to a DataFrame\n",
    "train_data_encoded_categorical = pd.DataFrame(\n",
    "    encoded_data.toarray(),\n",
    "    columns=one_hot_encoder.get_feature_names_out(categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeca286",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_scaled_without_categorical = train_data_scaled.drop(\n",
    "    columns=categorical_features\n",
    ").reset_index(drop=True)\n",
    "train_data_encoded = pd.concat(\n",
    "    [train_data_scaled_without_categorical, train_data_encoded_categorical], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "y_train = train_data_encoded[\"DefaulterType\"]\n",
    "X_train = train_data_encoded.drop([\"DefaulterType\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c590107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to one-hot encoded columns\n",
    "test_encoded_data = one_hot_encoder.transform(test_data_scaled[categorical_features])\n",
    "\n",
    "test_data_encoded_categorical = pd.DataFrame(\n",
    "    test_encoded_data.toarray(),\n",
    "    columns=one_hot_encoder.get_feature_names_out(categorical_features),\n",
    ")\n",
    "\n",
    "test_data_scaled_without_categorical = test_data_scaled.drop(\n",
    "    columns=categorical_features\n",
    ").reset_index(drop=True)\n",
    "test_data_encoded = pd.concat(\n",
    "    [test_data_scaled_without_categorical, test_data_encoded_categorical], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1104b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "y_test = test_data_encoded[\"DefaulterType\"]\n",
    "X_test = test_data_encoded.drop([\"DefaulterType\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d089670",
   "metadata": {},
   "source": [
    "### Step 3: Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07908215",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04729131",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b229bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "logreg_model = LogisticRegression()\n",
    "tree_model = DecisionTreeClassifier()\n",
    "rf_model = RandomForestClassifier()\n",
    "svm_model = SVC()\n",
    "\n",
    "# Train models\n",
    "logreg_model.fit(X_train, y_train)\n",
    "tree_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2ae8c",
   "metadata": {},
   "source": [
    "### Step 4: Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2452d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions\n",
    "logreg_pred = logreg_model.predict(X_test)\n",
    "tree_pred = tree_model.predict(X_test)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate models\n",
    "logreg_accuracy = accuracy_score(y_test, logreg_pred)\n",
    "tree_accuracy = accuracy_score(y_test, tree_pred)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "\n",
    "logreg_precision = precision_score(y_test, logreg_pred, average=\"micro\")\n",
    "tree_precision = precision_score(y_test, tree_pred, average=\"micro\")\n",
    "rf_precision = precision_score(y_test, rf_pred, average=\"micro\")\n",
    "svm_precision = precision_score(y_test, svm_pred, average=\"micro\")\n",
    "\n",
    "logreg_recall = recall_score(y_test, logreg_pred, average=\"micro\")\n",
    "tree_recall = recall_score(y_test, tree_pred, average=\"micro\")\n",
    "rf_recall = recall_score(y_test, rf_pred, average=\"micro\")\n",
    "svm_recall = recall_score(y_test, svm_pred, average=\"micro\")\n",
    "\n",
    "logreg_f1 = f1_score(y_test, logreg_pred, average=\"micro\")\n",
    "tree_f1 = f1_score(y_test, tree_pred, average=\"micro\")\n",
    "rf_f1 = f1_score(y_test, rf_pred, average=\"micro\")\n",
    "svm_f1 = f1_score(y_test, svm_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acd1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"logreg_recall: {logreg_recall}, tree_recall: {tree_recall}, rf_recall: {rf_recall}, svm_recall: {svm_recall}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec946b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"logreg_accuracy: {logreg_accuracy}, tree_accuracy: {tree_accuracy}, rf_accuracy: {rf_accuracy}, svm_accuracy: {svm_accuracy}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e62d3",
   "metadata": {},
   "source": [
    "### Step 5: Interpretability\n",
    "\n",
    "Explain the decision rules forthe models using their .feature*importances* and .coef\\_ attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584425f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances from the models\n",
    "tree_feature_importances = tree_model.feature_importances_\n",
    "rf_feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    range(len(feature_names)),\n",
    "    tree_feature_importances,\n",
    "    align=\"center\",\n",
    "    label=\"Decision Tree\",\n",
    ")\n",
    "plt.barh(\n",
    "    range(len(feature_names)),\n",
    "    rf_feature_importances,\n",
    "    align=\"center\",\n",
    "    label=\"Random Forest\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance from Decision Tree and Random Forest\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafbaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get absolute coefficients from the logistic regression model\n",
    "logreg_coeffs = np.exp(logreg_model.coef_[0])\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_names)), logreg_coeffs, align=\"center\")\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance from Logistic Regression\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce47b5",
   "metadata": {},
   "source": [
    "### Step 6: Parameter Tuning\n",
    "\n",
    "You can use techniques like grid search or random search to find optimal hyperparameters for each algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c8cad",
   "metadata": {},
   "source": [
    "Parameter tuning is a crucial step in optimizing the performance of machine learning models. Each algorithm has various parameters that can be adjusted to improve their predictive accuracy and generalization. In this step, you search for the best combination of parameter values to achieve the best model performance. Let's go through an explanation and example for each algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598dba6",
   "metadata": {},
   "source": [
    "1. Logistic Regression:\n",
    "\n",
    "For logistic regression, one common parameter is the regularization strength (C). A smaller C increases the regularization, which can prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\"C\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring=\"recall\")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameter\n",
    "best_C = grid_search.best_params_[\"C\"]\n",
    "\n",
    "# Train a Logistic Regression model with the best parameter\n",
    "best_logreg_model = LogisticRegression(C=best_C)\n",
    "best_logreg_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_logreg_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the best C value and metrics\n",
    "print(f\"Best C parameter: {best_C}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e85ecc4",
   "metadata": {},
   "source": [
    "2. Decision Trees:\n",
    "\n",
    "For decision trees, an important parameter is the maximum depth of the tree (max_depth). A deeper tree can lead to overfitting, so you need to find an optimal value for this parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"max_depth\": [None, 5, 10, 15, 20]}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(), param_grid, cv=5, scoring=\"recall\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_max_depth = grid_search.best_params_[\"max_depth\"]\n",
    "\n",
    "best_tree_model = DecisionTreeClassifier(max_depth=best_max_depth)\n",
    "best_tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb71814",
   "metadata": {},
   "source": [
    "3. Random Forest:\n",
    "\n",
    "Random Forests have parameters like the number of trees (n_estimators) and the maximum depth of each tree (max_depth). Tuning these parameters can improve the performance of the ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201aab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [50, 100, 150], \"max_depth\": [None, 10, 20]}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(), param_grid, cv=5, scoring=\"recall\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_n_estimators = grid_search.best_params_[\"n_estimators\"]\n",
    "best_max_depth = grid_search.best_params_[\"max_depth\"]\n",
    "\n",
    "best_rf_model = RandomForestClassifier(\n",
    "    n_estimators=best_n_estimators, max_depth=best_max_depth\n",
    ")\n",
    "best_rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818f41b",
   "metadata": {},
   "source": [
    "4. Support Vector Machines (SVM):\n",
    "\n",
    "For SVMs, parameters like the regularization parameter (C) and the choice of kernel (kernel) are important to tune. The kernel can be linear, polynomial, or radial basis function (RBF).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cccff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"poly\", \"rbf\"]}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring=\"recall\")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_C = grid_search.best_params_[\"C\"]\n",
    "best_kernel = grid_search.best_params_[\"kernel\"]\n",
    "\n",
    "best_svm_model = SVC(C=best_C, kernel=best_kernel)\n",
    "best_svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390399e",
   "metadata": {},
   "source": [
    "In each example, we're using GridSearchCV to perform a grid search over the specified parameter values. The cv parameter indicates the number of folds in cross-validation, and scoring specifies the evaluation metric. After finding the best parameter combination, a new model is trained using the best parameters.\n",
    "\n",
    "Remember that these are simplified examples, and you can explore more parameters and values to fine-tune your models further. Also, be mindful of the computational cost of exhaustive grid searches, as they can become time-consuming with larger datasets or complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38073b81",
   "metadata": {},
   "source": [
    "## Step 7: Conclusion\n",
    "\n",
    "Summarize the results based on accuracy, precision, recall, and F1-score metrics and discuss which algorithm might be the most suitable for this loan approval prediction problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28ff08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
